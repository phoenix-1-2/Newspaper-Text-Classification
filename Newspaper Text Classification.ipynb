{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from nltk import pos_tag\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import NaiveBayesClassifier\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = data['data']\n",
    "target = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = stopwords.words('english')\n",
    "stopwords=[\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\",\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\",\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\",\n",
    "\"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\",\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\",'co','op','research-articl', 'pagecount','cit','ibid','les','le','au','que','est','pas','vol','el','los','pp','u201d','well-b', 'http', 'volumtype', 'par', '0o', '0s', '3a', '3b', '3d', '6b', '6o', 'a1', 'a2', 'a3', 'a4', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'aj', 'al', 'an', 'ao', 'ap', 'ar', 'av', 'aw', 'ax', 'ay', 'az', 'b1', 'b2', 'b3', 'ba', 'bc', 'bd', 'be', 'bi', 'bj', 'bk', 'bl', 'bn', 'bp', 'br', 'bs', 'bt', 'bu', 'bx', 'c1', 'c2', 'c3', 'cc', 'cd', 'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'cl', 'cm', 'cn', 'cp', 'cq', 'cr', 'cs', 'ct', 'cu', 'cv', 'cx', 'cy', 'cz', 'd2', 'da', 'dc', 'dd', 'de', 'df', 'di', 'dj', 'dk', 'dl', 'do', 'dp', 'dr', 'ds', 'dt', 'du', 'dx', 'dy', 'e2', 'e3', 'ea', 'ec', 'ed', 'ee', 'ef', 'ei', 'ej', 'el', 'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ex', 'ey', 'f2', 'fa', 'fc', 'ff', 'fi', 'fj', 'fl', 'fn', 'fo', 'fr', 'fs', 'ft', 'fu', 'fy', 'ga', 'ge', 'gi', 'gj', 'gl', 'go', 'gr', 'gs', 'gy', 'h2', 'h3', 'hh', 'hi', 'hj', 'ho', 'hr', 'hs', 'hu', 'hy', 'i', 'i2', 'i3', 'i4', 'i6', 'i7', 'i8', 'ia', 'ib', 'ic', 'ie', 'ig', 'ih', 'ii', 'ij', 'il', 'in', 'io', 'ip', 'iq', 'ir', 'iv', 'ix', 'iy', 'iz', 'jj', 'jr', 'js', 'jt', 'ju', 'ke', 'kg', 'kj', 'km', 'ko', 'l2', 'la', 'lb', 'lc', 'lf', 'lj', 'ln', 'lo', 'lr', 'ls', 'lt', 'm2', 'ml', 'mn', 'mo', 'ms', 'mt', 'mu', 'n2', 'nc', 'nd', 'ne', 'ng', 'ni', 'nj', 'nl', 'nn', 'nr', 'ns', 'nt', 'ny', 'oa', 'ob', 'oc', 'od', 'of', 'og', 'oi', 'oj', 'ol', 'om', 'on', 'oo', 'oq', 'or', 'os', 'ot', 'ou', 'ow', 'ox', 'oz', 'p1', 'p2', 'p3', 'pc', 'pd', 'pe', 'pf', 'ph', 'pi', 'pj', 'pk', 'pl', 'pm', 'pn', 'po', 'pq', 'pr', 'ps', 'pt', 'pu', 'py', 'qj', 'qu', 'r2', 'ra', 'rc', 'rd', 'rf', 'rh', 'ri', 'rj', 'rl', 'rm', 'rn', 'ro', 'rq', 'rr', 'rs', 'rt', 'ru', 'rv', 'ry', 's2', 'sa', 'sc', 'sd', 'se', 'sf', 'si', 'sj', 'sl', 'sm', 'sn', 'sp', 'sq', 'sr', 'ss', 'st', 'sy', 'sz', 't1', 't2', 't3', 'tb', 'tc', 'td', 'te', 'tf', 'th', 'ti', 'tj', 'tl', 'tm', 'tn', 'tp', 'tq', 'tr', 'ts', 'tt', 'tv', 'tx', 'ue', 'ui', 'uj', 'uk', 'um', 'un', 'uo', 'ur', 'ut', 'va', 'wa', 'vd', 'wi', 'vj', 'vo', 'wo', 'vq', 'vt', 'vu', 'x1', 'x2', 'x3', 'xf', 'xi', 'xj', 'xk', 'xl', 'xn', 'xo', 'xs', 'xt', 'xv', 'xx', 'y2', 'yj', 'yl', 'yr', 'ys', 'yt', 'zi', 'zz','--']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords += list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for doc in news_data:\n",
    "    documents.append(word_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " ':',\n",
       " 'lerxst',\n",
       " '@',\n",
       " 'wam.umd.edu',\n",
       " '(',\n",
       " 'where',\n",
       " \"'s\",\n",
       " 'my',\n",
       " 'thing',\n",
       " ')',\n",
       " 'Subject',\n",
       " ':',\n",
       " 'WHAT',\n",
       " 'car',\n",
       " 'is',\n",
       " 'this',\n",
       " '!',\n",
       " '?',\n",
       " 'Nntp-Posting-Host',\n",
       " ':',\n",
       " 'rac3.wam.umd.edu',\n",
       " 'Organization',\n",
       " ':',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Maryland',\n",
       " ',',\n",
       " 'College',\n",
       " 'Park',\n",
       " 'Lines',\n",
       " ':',\n",
       " '15',\n",
       " 'I',\n",
       " 'was',\n",
       " 'wondering',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'out',\n",
       " 'there',\n",
       " 'could',\n",
       " 'enlighten',\n",
       " 'me',\n",
       " 'on',\n",
       " 'this',\n",
       " 'car',\n",
       " 'I',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'other',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'a',\n",
       " '2-door',\n",
       " 'sports',\n",
       " 'car',\n",
       " ',',\n",
       " 'looked',\n",
       " 'to',\n",
       " 'be',\n",
       " 'from',\n",
       " 'the',\n",
       " 'late',\n",
       " '60s/',\n",
       " 'early',\n",
       " '70s',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'called',\n",
       " 'a',\n",
       " 'Bricklin',\n",
       " '.',\n",
       " 'The',\n",
       " 'doors',\n",
       " 'were',\n",
       " 'really',\n",
       " 'small',\n",
       " '.',\n",
       " 'In',\n",
       " 'addition',\n",
       " ',',\n",
       " 'the',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'was',\n",
       " 'separate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'body',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'all',\n",
       " 'I',\n",
       " 'know',\n",
       " '.',\n",
       " 'If',\n",
       " 'anyone',\n",
       " 'can',\n",
       " 'tellme',\n",
       " 'a',\n",
       " 'model',\n",
       " 'name',\n",
       " ',',\n",
       " 'engine',\n",
       " 'specs',\n",
       " ',',\n",
       " 'years',\n",
       " 'of',\n",
       " 'production',\n",
       " ',',\n",
       " 'where',\n",
       " 'this',\n",
       " 'car',\n",
       " 'is',\n",
       " 'made',\n",
       " ',',\n",
       " 'history',\n",
       " ',',\n",
       " 'or',\n",
       " 'whatever',\n",
       " 'info',\n",
       " 'you',\n",
       " 'have',\n",
       " 'on',\n",
       " 'this',\n",
       " 'funky',\n",
       " 'looking',\n",
       " 'car',\n",
       " ',',\n",
       " 'please',\n",
       " 'e-mail',\n",
       " '.',\n",
       " 'Thanks',\n",
       " ',',\n",
       " '-',\n",
       " 'IL',\n",
       " '--',\n",
       " '--',\n",
       " 'brought',\n",
       " 'to',\n",
       " 'you',\n",
       " 'by',\n",
       " 'your',\n",
       " 'neighborhood',\n",
       " 'Lerxst',\n",
       " '--',\n",
       " '--']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('better', 'RBR')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['better'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lematizer = WordNetLemmatizer()\n",
    "def small_tag(tag):\n",
    "    if tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('A'):\n",
    "        return wordnet.ADV\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    return wordnet.NOUN \n",
    "def clean(doc):\n",
    "    c =[]\n",
    "    for word in doc:\n",
    "        if (word.lower() not in stopwords) and (not word.isnumeric()):\n",
    "            pos = pos_tag([word])[0][1]\n",
    "            c.append(lematizer.lemmatize(word, small_tag(pos)).lower())\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lerxst',\n",
       " 'wam.umd.edu',\n",
       " \"'s\",\n",
       " 'thing',\n",
       " 'subject',\n",
       " 'car',\n",
       " 'nntp-posting-host',\n",
       " 'rac3.wam.umd.edu',\n",
       " 'organization',\n",
       " 'university',\n",
       " 'maryland',\n",
       " 'college',\n",
       " 'park',\n",
       " 'lines',\n",
       " 'wonder',\n",
       " 'enlighten',\n",
       " 'car',\n",
       " 'day',\n",
       " '2-door',\n",
       " 'sport',\n",
       " 'car',\n",
       " 'look',\n",
       " 'late',\n",
       " '60s/',\n",
       " 'early',\n",
       " '70',\n",
       " 'call',\n",
       " 'bricklin',\n",
       " 'door',\n",
       " 'small',\n",
       " 'addition',\n",
       " 'bumper',\n",
       " 'separate',\n",
       " 'rest',\n",
       " 'body',\n",
       " 'tellme',\n",
       " 'model',\n",
       " 'engine',\n",
       " 'spec',\n",
       " 'year',\n",
       " 'production',\n",
       " 'car',\n",
       " 'history',\n",
       " 'info',\n",
       " 'funky',\n",
       " 'car',\n",
       " 'e-mail',\n",
       " 'brought',\n",
       " 'neighborhood',\n",
       " 'lerxst']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'000'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_documents=[ clean(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lerxst',\n",
       " 'wam.umd.edu',\n",
       " \"'s\",\n",
       " 'thing',\n",
       " 'subject',\n",
       " 'car',\n",
       " 'nntp-posting-host',\n",
       " 'rac3.wam.umd.edu',\n",
       " 'organization',\n",
       " 'university',\n",
       " 'maryland',\n",
       " 'college',\n",
       " 'park',\n",
       " 'lines',\n",
       " 'wonder',\n",
       " 'enlighten',\n",
       " 'car',\n",
       " 'day',\n",
       " '2-door',\n",
       " 'sport',\n",
       " 'car',\n",
       " 'look',\n",
       " 'late',\n",
       " '60s/',\n",
       " 'early',\n",
       " '70',\n",
       " 'call',\n",
       " 'bricklin',\n",
       " 'door',\n",
       " 'small',\n",
       " 'addition',\n",
       " 'bumper',\n",
       " 'separate',\n",
       " 'rest',\n",
       " 'body',\n",
       " 'tellme',\n",
       " 'model',\n",
       " 'engine',\n",
       " 'spec',\n",
       " 'year',\n",
       " 'production',\n",
       " 'car',\n",
       " 'history',\n",
       " 'info',\n",
       " 'funky',\n",
       " 'car',\n",
       " 'e-mail',\n",
       " 'brought',\n",
       " 'neighborhood',\n",
       " 'lerxst']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit([ (' '.join(doc)).strip()  for doc in clean_documents ] )\n",
    "X = cv.transform([ (' '.join(doc)).strip()  for doc in clean_documents ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '0d',\n",
       " '0g',\n",
       " '0i',\n",
       " '0q',\n",
       " '0t',\n",
       " '10',\n",
       " '100',\n",
       " '11',\n",
       " '12',\n",
       " '128',\n",
       " '129',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '1992',\n",
       " '1993',\n",
       " '1993apr13',\n",
       " '1993apr14',\n",
       " '1993apr15',\n",
       " '1993apr16',\n",
       " '1993apr17',\n",
       " '1993apr18',\n",
       " '1993apr19',\n",
       " '1993apr20',\n",
       " '1993apr21',\n",
       " '1993apr5',\n",
       " '1993apr6',\n",
       " '1d',\n",
       " '1d9',\n",
       " '1eq',\n",
       " '1f',\n",
       " '1st',\n",
       " '1t',\n",
       " '20',\n",
       " '200',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '241',\n",
       " '24e',\n",
       " '25',\n",
       " '250',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '2d',\n",
       " '2di',\n",
       " '2nd',\n",
       " '2tm',\n",
       " '30',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '34u',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '3dy',\n",
       " '3l',\n",
       " '3rd',\n",
       " '3t',\n",
       " '40',\n",
       " '408',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '4t',\n",
       " '4u',\n",
       " '50',\n",
       " '500',\n",
       " '51',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '5u',\n",
       " '60',\n",
       " '61',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '6e',\n",
       " '6ei',\n",
       " '6g',\n",
       " '6t',\n",
       " '6um',\n",
       " '70',\n",
       " '71',\n",
       " '72',\n",
       " '75',\n",
       " '75u',\n",
       " '76',\n",
       " '7ey',\n",
       " '7ez',\n",
       " '7kn',\n",
       " '7u',\n",
       " '80',\n",
       " '800',\n",
       " '81',\n",
       " '86',\n",
       " '88',\n",
       " '89',\n",
       " '8n',\n",
       " '90',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '95',\n",
       " '9760',\n",
       " '9f',\n",
       " '9v',\n",
       " '__',\n",
       " '___',\n",
       " '____',\n",
       " '_____',\n",
       " '_o',\n",
       " 'a7',\n",
       " 'a86',\n",
       " 'aaron',\n",
       " 'abc',\n",
       " 'ability',\n",
       " 'abortion',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abuse',\n",
       " 'ac',\n",
       " 'academic',\n",
       " 'acc',\n",
       " 'accelerator',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accepted',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accomplish',\n",
       " 'account',\n",
       " 'accurate',\n",
       " 'accuse',\n",
       " 'achieve',\n",
       " 'acid',\n",
       " 'acns',\n",
       " 'acs',\n",
       " 'act',\n",
       " 'action',\n",
       " 'active',\n",
       " 'activity',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'adaptec',\n",
       " 'adapter',\n",
       " 'add',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'adl',\n",
       " 'administration',\n",
       " 'admit',\n",
       " 'adobe',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advice',\n",
       " 'advocate',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'age',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreement',\n",
       " 'ahead',\n",
       " 'ahl',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aids',\n",
       " 'aim',\n",
       " 'air',\n",
       " 'al',\n",
       " 'alan',\n",
       " 'alaska',\n",
       " 'alchemy',\n",
       " 'algorithm',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allan',\n",
       " 'allen',\n",
       " 'allow',\n",
       " 'alomar',\n",
       " 'alt',\n",
       " 'alternative',\n",
       " 'ama',\n",
       " 'amanda',\n",
       " 'amateur',\n",
       " 'amaze',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'ames',\n",
       " 'amiga',\n",
       " 'amp',\n",
       " 'analog',\n",
       " 'analysis',\n",
       " 'ancient',\n",
       " 'and',\n",
       " 'anderson',\n",
       " 'andrew',\n",
       " 'andy',\n",
       " 'angeles',\n",
       " 'angle',\n",
       " 'animal',\n",
       " 'animation',\n",
       " 'ann',\n",
       " 'announce',\n",
       " 'announcement',\n",
       " 'annual',\n",
       " 'anonymity',\n",
       " 'anonymous',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'antenna',\n",
       " 'anti',\n",
       " 'apartment',\n",
       " 'apc',\n",
       " 'apollo',\n",
       " 'app',\n",
       " 'apparent',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'approach',\n",
       " 'approve',\n",
       " 'apps',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'arab',\n",
       " 'arabs',\n",
       " 'arbor',\n",
       " 'arc',\n",
       " 'archive',\n",
       " 'area',\n",
       " 'argic',\n",
       " 'argue',\n",
       " 'argument',\n",
       " 'arizona',\n",
       " 'arm',\n",
       " 'armenia',\n",
       " 'armenian',\n",
       " 'armenians',\n",
       " 'army',\n",
       " 'arrest',\n",
       " 'arrive',\n",
       " 'arrogance',\n",
       " 'art',\n",
       " 'article',\n",
       " 'artificial',\n",
       " 'as',\n",
       " 'ask',\n",
       " 'aspect',\n",
       " 'assault',\n",
       " 'assembly',\n",
       " 'assert',\n",
       " 'assertion',\n",
       " 'assist',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'assumption',\n",
       " 'astro',\n",
       " 'astronomy',\n",
       " 'at',\n",
       " 'atf',\n",
       " 'atheism',\n",
       " 'atheist',\n",
       " 'atheists',\n",
       " 'athena',\n",
       " 'athens',\n",
       " 'athos',\n",
       " 'ati',\n",
       " 'atlanta',\n",
       " 'att',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attorney',\n",
       " 'attribute',\n",
       " 'au',\n",
       " 'audio',\n",
       " 'august',\n",
       " 'aurora',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'auto',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'autos',\n",
       " 'ave',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'awful',\n",
       " 'ax',\n",
       " 'azerbaijan',\n",
       " 'azerbaijani',\n",
       " 'b4q',\n",
       " 'b8',\n",
       " 'b8e',\n",
       " 'b8f',\n",
       " 'b9r',\n",
       " 'baalke',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'background',\n",
       " 'backup',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'baker',\n",
       " 'balance',\n",
       " 'ball',\n",
       " 'baltimore',\n",
       " 'ban',\n",
       " 'band',\n",
       " 'bank',\n",
       " 'banks',\n",
       " 'bar',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'bat',\n",
       " 'batf',\n",
       " 'battery',\n",
       " 'battle',\n",
       " 'bay',\n",
       " 'bbs',\n",
       " 'bc',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'bear',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'begin',\n",
       " 'behanna',\n",
       " 'behavior',\n",
       " 'being',\n",
       " 'belief',\n",
       " 'believe',\n",
       " 'believer',\n",
       " 'bell',\n",
       " 'bellcore',\n",
       " 'belong',\n",
       " 'benefit',\n",
       " 'berkeley',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'bh',\n",
       " 'bhj',\n",
       " 'bias',\n",
       " 'bible',\n",
       " 'biblical',\n",
       " 'big',\n",
       " 'bike',\n",
       " 'bill',\n",
       " 'billion',\n",
       " 'binary',\n",
       " 'bios',\n",
       " 'birth',\n",
       " 'bit',\n",
       " 'bitmap',\n",
       " 'bitnet',\n",
       " 'biz',\n",
       " 'black',\n",
       " 'blame',\n",
       " 'blind',\n",
       " 'block',\n",
       " 'blood',\n",
       " 'blow',\n",
       " 'blue',\n",
       " 'blues',\n",
       " 'bmp',\n",
       " 'bmw',\n",
       " 'bnr',\n",
       " 'board',\n",
       " 'bob',\n",
       " 'bobby',\n",
       " 'body',\n",
       " 'boeing',\n",
       " 'bomb',\n",
       " 'bontchev',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boot',\n",
       " 'border',\n",
       " 'born',\n",
       " 'bos',\n",
       " 'bosnia',\n",
       " 'boston',\n",
       " 'bother',\n",
       " 'bought',\n",
       " 'boulder',\n",
       " 'bound',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'br',\n",
       " 'brad',\n",
       " 'bradley',\n",
       " 'brain',\n",
       " 'brake',\n",
       " 'branch',\n",
       " 'brand',\n",
       " 'braves',\n",
       " 'break',\n",
       " 'breaker',\n",
       " 'brian',\n",
       " 'bring',\n",
       " 'british',\n",
       " 'broadcast',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'bruce',\n",
       " 'bruins',\n",
       " 'bryan',\n",
       " 'btw',\n",
       " 'bu',\n",
       " 'buck',\n",
       " 'budget',\n",
       " 'buf',\n",
       " 'buffalo',\n",
       " 'buffer',\n",
       " 'bug',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bull',\n",
       " 'bunch',\n",
       " 'bureau',\n",
       " 'burn',\n",
       " 'burning',\n",
       " 'bus',\n",
       " 'bush',\n",
       " 'business',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'bxn',\n",
       " 'by',\n",
       " 'byte',\n",
       " 'c8',\n",
       " 'c8v',\n",
       " 'c_',\n",
       " 'ca',\n",
       " 'cable',\n",
       " 'cache',\n",
       " 'cactus',\n",
       " 'cal',\n",
       " 'calgary',\n",
       " 'california',\n",
       " 'call',\n",
       " 'callison',\n",
       " 'caltech',\n",
       " 'cam',\n",
       " 'cambridge',\n",
       " 'camera',\n",
       " 'camp',\n",
       " 'campaign',\n",
       " 'canada',\n",
       " 'canadian',\n",
       " 'cancer',\n",
       " 'candida',\n",
       " 'candidate',\n",
       " 'canon',\n",
       " 'cap',\n",
       " 'capability',\n",
       " 'capable',\n",
       " 'capital',\n",
       " 'captain',\n",
       " 'capture',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'career',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'carl',\n",
       " 'carleton',\n",
       " 'carnegie',\n",
       " 'carolina',\n",
       " 'carry',\n",
       " 'carson',\n",
       " 'cartridge',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cat',\n",
       " 'catalog',\n",
       " 'catch',\n",
       " 'category',\n",
       " 'catholic',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'cb',\n",
       " 'cc',\n",
       " 'cco',\n",
       " 'ccwf',\n",
       " 'cd',\n",
       " 'cdt',\n",
       " 'cell',\n",
       " 'cellular',\n",
       " 'center',\n",
       " 'central',\n",
       " 'centre',\n",
       " 'centris',\n",
       " 'century',\n",
       " 'ch',\n",
       " 'chain',\n",
       " 'challenge',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'channel',\n",
       " 'chapter',\n",
       " 'char',\n",
       " 'character',\n",
       " 'charge',\n",
       " 'charles',\n",
       " 'charlie',\n",
       " 'cheap',\n",
       " 'cheaper',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'cheers',\n",
       " 'chem',\n",
       " 'chemical',\n",
       " 'chemistry',\n",
       " 'chi',\n",
       " 'chicago',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'chinese',\n",
       " 'chip',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'chosen',\n",
       " 'chris',\n",
       " 'christ',\n",
       " 'christian',\n",
       " 'christianity',\n",
       " 'christians',\n",
       " 'christopher',\n",
       " 'chuck',\n",
       " 'church',\n",
       " 'chz',\n",
       " 'cipher',\n",
       " 'circle',\n",
       " 'circuit',\n",
       " 'circumstance',\n",
       " 'cis',\n",
       " 'cite',\n",
       " 'citizen',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'civilian',\n",
       " 'ck',\n",
       " 'claim',\n",
       " 'claimed',\n",
       " 'claremont',\n",
       " 'clark',\n",
       " 'clarkson',\n",
       " 'class',\n",
       " 'classify',\n",
       " 'clayton',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'cleveland',\n",
       " 'clh',\n",
       " 'client',\n",
       " 'clinton',\n",
       " 'clipper',\n",
       " 'clock',\n",
       " 'clone',\n",
       " 'close',\n",
       " 'closely',\n",
       " 'closer',\n",
       " 'club',\n",
       " 'cmu',\n",
       " 'co',\n",
       " 'coach',\n",
       " 'code',\n",
       " 'col',\n",
       " 'cold',\n",
       " 'collect',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'color',\n",
       " 'colorado',\n",
       " 'colormap',\n",
       " 'colostate',\n",
       " 'colour',\n",
       " 'columbia',\n",
       " 'column',\n",
       " 'com',\n",
       " 'combination',\n",
       " 'combine',\n",
       " 'come',\n",
       " 'comet',\n",
       " 'comm',\n",
       " 'command',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'commit',\n",
       " 'committee',\n",
       " 'common',\n",
       " 'communication',\n",
       " 'communications',\n",
       " 'community',\n",
       " 'comp',\n",
       " 'company',\n",
       " 'compare',\n",
       " 'comparison',\n",
       " 'compatible',\n",
       " 'compete',\n",
       " 'compile',\n",
       " 'compiler',\n",
       " 'complain',\n",
       " 'complaint',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'component',\n",
       " 'compound',\n",
       " 'comprehensive',\n",
       " 'compress',\n",
       " 'compression',\n",
       " 'compromise',\n",
       " 'compuserve',\n",
       " 'compute',\n",
       " 'computer',\n",
       " 'computing',\n",
       " 'concentrate',\n",
       " 'concept',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'conclude',\n",
       " 'conclusion',\n",
       " 'condemn',\n",
       " 'condition',\n",
       " 'conduct',\n",
       " 'conference',\n",
       " 'configuration',\n",
       " 'confirm',\n",
       " 'conflict',\n",
       " 'confuse',\n",
       " 'congress',\n",
       " 'connect',\n",
       " 'connection',\n",
       " 'connector',\n",
       " 'consequence',\n",
       " 'consider',\n",
       " 'consideration',\n",
       " 'consistent',\n",
       " 'constant',\n",
       " 'constitution',\n",
       " 'constitutional',\n",
       " 'contact',\n",
       " 'content',\n",
       " 'contest',\n",
       " 'context',\n",
       " 'continue',\n",
       " 'continued',\n",
       " 'contract',\n",
       " 'contradiction',\n",
       " 'contrary',\n",
       " 'contrib',\n",
       " 'contribute',\n",
       " 'control',\n",
       " 'controller',\n",
       " 'conversation',\n",
       " 'conversion',\n",
       " 'convert',\n",
       " 'converter',\n",
       " 'convex',\n",
       " 'convince',\n",
       " 'cook',\n",
       " 'cool',\n",
       " 'cop',\n",
       " 'copy',\n",
       " 'copyright',\n",
       " 'core',\n",
       " 'cornell',\n",
       " 'corner',\n",
       " 'corp',\n",
       " 'corporation',\n",
       " 'correct',\n",
       " 'correction',\n",
       " 'correctly',\n",
       " 'cost',\n",
       " 'council',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'country',\n",
       " 'county',\n",
       " 'couple',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'coverage',\n",
       " 'cpr',\n",
       " 'cpu',\n",
       " 'crack',\n",
       " 'craft',\n",
       " 'craig',\n",
       " 'cramer',\n",
       " 'crap',\n",
       " 'crash',\n",
       " 'cray',\n",
       " 'crazy',\n",
       " 'create',\n",
       " 'creation',\n",
       " 'credit',\n",
       " 'crime',\n",
       " 'criminal',\n",
       " 'critical',\n",
       " 'cross',\n",
       " 'crowd',\n",
       " 'crypt',\n",
       " 'crypto',\n",
       " 'cryptography',\n",
       " 'cs',\n",
       " 'csc',\n",
       " 'csd',\n",
       " 'cso',\n",
       " 'cubs',\n",
       " 'cult',\n",
       " 'cultural',\n",
       " 'culture',\n",
       " 'cunixb',\n",
       " 'cup',\n",
       " 'cure',\n",
       " 'curious',\n",
       " 'current',\n",
       " 'cursor',\n",
       " 'custom',\n",
       " 'customer',\n",
       " 'cut',\n",
       " 'cwru',\n",
       " 'cx',\n",
       " 'cycle',\n",
       " 'cylinder',\n",
       " 'cyprus',\n",
       " 'd9',\n",
       " 'daily',\n",
       " 'dale',\n",
       " 'dallas',\n",
       " 'damage',\n",
       " 'damn',\n",
       " 'dan',\n",
       " 'danger',\n",
       " 'dangerous',\n",
       " 'daniel',\n",
       " 'dare',\n",
       " 'dark',\n",
       " 'dartmouth',\n",
       " 'data',\n",
       " 'database',\n",
       " 'date',\n",
       " 'daughter',\n",
       " 'dave',\n",
       " 'david',\n",
       " 'davidian',\n",
       " 'davis',\n",
       " 'day',\n",
       " 'db',\n",
       " 'dc',\n",
       " 'de',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dealer',\n",
       " 'dean',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'debate',\n",
       " 'dec',\n",
       " 'decade',\n",
       " 'december',\n",
       " 'decent',\n",
       " 'decide',\n",
       " 'decision',\n",
       " 'declare',\n",
       " 'decrypt',\n",
       " 'deep',\n",
       " 'default',\n",
       " 'defend',\n",
       " 'defense',\n",
       " 'defensive',\n",
       " 'define',\n",
       " 'definition',\n",
       " 'degree',\n",
       " 'delete',\n",
       " 'deliver',\n",
       " 'demand',\n",
       " 'demo',\n",
       " 'democracy',\n",
       " 'demon',\n",
       " 'demonstrate',\n",
       " 'denver',\n",
       " 'deny',\n",
       " 'department',\n",
       " 'depend',\n",
       " 'depends',\n",
       " 'dept',\n",
       " 'depth',\n",
       " 'des',\n",
       " 'description',\n",
       " 'deserve',\n",
       " 'design',\n",
       " 'desire',\n",
       " 'desktop',\n",
       " 'destroy',\n",
       " 'destroyed',\n",
       " 'det',\n",
       " 'detail',\n",
       " 'detailed',\n",
       " 'detect',\n",
       " 'detector',\n",
       " 'determine',\n",
       " 'detroit',\n",
       " 'develop',\n",
       " 'developed',\n",
       " 'development',\n",
       " 'device',\n",
       " 'devils',\n",
       " 'dg',\n",
       " 'diamond',\n",
       " 'dick',\n",
       " 'dictionary',\n",
       " 'die',\n",
       " 'diego',\n",
       " 'diet',\n",
       " 'difference',\n",
       " 'difficult',\n",
       " 'digest',\n",
       " 'digex',\n",
       " 'digital',\n",
       " 'direct',\n",
       " 'direction',\n",
       " 'directly',\n",
       " 'director',\n",
       " 'directory',\n",
       " 'disagree',\n",
       " 'disclaimer',\n",
       " 'discover',\n",
       " 'discus',\n",
       " 'discuss',\n",
       " 'discussion',\n",
       " 'disease',\n",
       " 'disk',\n",
       " 'display',\n",
       " 'dispute',\n",
       " 'distance',\n",
       " 'distribute',\n",
       " 'distribution',\n",
       " 'divine',\n",
       " 'division',\n",
       " 'dma',\n",
       " 'do',\n",
       " 'doc',\n",
       " 'doctor',\n",
       " 'doctrine',\n",
       " 'document',\n",
       " 'documentation',\n",
       " 'dod',\n",
       " 'dog',\n",
       " 'dollar',\n",
       " 'domain',\n",
       " 'door',\n",
       " 'dos',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'doug',\n",
       " 'douglas',\n",
       " 'dozen',\n",
       " 'dr',\n",
       " 'draft',\n",
       " 'drag',\n",
       " 'draw',\n",
       " 'dream',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'driven',\n",
       " 'driver',\n",
       " 'drop',\n",
       " 'drug',\n",
       " 'dry',\n",
       " 'dseg',\n",
       " 'du',\n",
       " 'dual',\n",
       " 'duke',\n",
       " 'duo',\n",
       " 'duty',\n",
       " 'dyer',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'easily',\n",
       " 'east',\n",
       " 'eastern',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'ece',\n",
       " 'echo',\n",
       " 'ecn',\n",
       " 'economic',\n",
       " 'economy',\n",
       " 'ed',\n",
       " 'edge',\n",
       " 'edit',\n",
       " 'edition',\n",
       " 'editor',\n",
       " 'edmonton',\n",
       " 'edu',\n",
       " 'education',\n",
       " 'educational',\n",
       " 'edward',\n",
       " 'ee',\n",
       " 'eff',\n",
       " 'effect',\n",
       " 'effective',\n",
       " 'effectively',\n",
       " 'effort',\n",
       " 'eh',\n",
       " 'eisa',\n",
       " 'electric',\n",
       " 'electrical',\n",
       " 'electronic',\n",
       " 'electronics',\n",
       " 'element',\n",
       " 'eliminate',\n",
       " 'em',\n",
       " 'email',\n",
       " 'empire',\n",
       " 'employer',\n",
       " 'encounter',\n",
       " 'encourage',\n",
       " 'encrypt',\n",
       " 'encryption',\n",
       " 'end',\n",
       " 'enemy',\n",
       " 'energy',\n",
       " 'enforcement',\n",
       " 'eng',\n",
       " 'engage',\n",
       " 'engin',\n",
       " 'engine',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.791445740544362"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model1 = MultinomialNB()\n",
    "model1.fit(X_train,Y_train)\n",
    "model1.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8055850123718629"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model2 = RandomForestClassifier()\n",
    "model2.fit(X_train,Y_train)\n",
    "model2.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8130081300813008"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = RandomForestClassifier(n_estimators=1000)\n",
    "model2.fit(X_train,Y_train)\n",
    "model2.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824673029338989"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model3 = LogisticRegression(max_iter=1000)\n",
    "model3.fit(X_train,Y_train)\n",
    "model3.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# model4 = SVC()\n",
    "# model4.fit(X_train,Y_train)\n",
    "# model4.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5072463768115942"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model4 = KNeighborsClassifier()\n",
    "model4.fit(X_train,Y_train)\n",
    "model4.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8485, 3000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=0\n",
    "curr =0;\n",
    "total = sum( pca.explained_variance_)\n",
    "while curr/total<0.99:\n",
    "    curr+=pca.explained_variance_[k]\n",
    "    k+=1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=k)\n",
    "X_train_new = pca.fit_transform(X_train)\n",
    "X_test_new  = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07917992223400495"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = RandomForestClassifier(n_estimators=1000)\n",
    "model2.fit(X_train_new,Y_train)\n",
    "model2.score(X_test_new,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.048427006009190525"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = LogisticRegression(max_iter=2000)\n",
    "model3.fit(X_train_new,Y_train)\n",
    "model3.score(X_test_new,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(256,activation='sigmoid'))\n",
    "model.add(Dropout(0.3) )\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(20,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "85/85 [==============================] - 3s 38ms/step - loss: 0.2279 - accuracy: 0.0542\n",
      "Epoch 2/20\n",
      "85/85 [==============================] - 3s 37ms/step - loss: 0.1747 - accuracy: 0.2684\n",
      "Epoch 3/20\n",
      "85/85 [==============================] - 3s 38ms/step - loss: 0.0992 - accuracy: 0.7130\n",
      "Epoch 4/20\n",
      "85/85 [==============================] - 3s 37ms/step - loss: 0.0411 - accuracy: 0.9037\n",
      "Epoch 5/20\n",
      "85/85 [==============================] - 3s 38ms/step - loss: 0.0158 - accuracy: 0.9723\n",
      "Epoch 6/20\n",
      "85/85 [==============================] - 3s 37ms/step - loss: 0.0062 - accuracy: 0.9916\n",
      "Epoch 7/20\n",
      "85/85 [==============================] - 3s 38ms/step - loss: 0.0028 - accuracy: 0.9972\n",
      "Epoch 8/20\n",
      "85/85 [==============================] - 3s 37ms/step - loss: 0.0016 - accuracy: 0.9987\n",
      "Epoch 9/20\n",
      "85/85 [==============================] - 3s 38ms/step - loss: 9.7767e-04 - accuracy: 0.9991\n",
      "Epoch 10/20\n",
      "85/85 [==============================] - 3s 38ms/step - loss: 7.2827e-04 - accuracy: 0.9993\n",
      "Epoch 11/20\n",
      "85/85 [==============================] - 3s 39ms/step - loss: 6.0602e-04 - accuracy: 0.9994\n",
      "Epoch 12/20\n",
      "85/85 [==============================] - 3s 37ms/step - loss: 4.3120e-04 - accuracy: 0.9995\n",
      "Epoch 13/20\n",
      "85/85 [==============================] - 3s 37ms/step - loss: 3.8099e-04 - accuracy: 0.9994\n",
      "Epoch 14/20\n",
      "85/85 [==============================] - 3s 41ms/step - loss: 3.3181e-04 - accuracy: 0.9993\n",
      "Epoch 15/20\n",
      "85/85 [==============================] - 4s 49ms/step - loss: 3.4291e-04 - accuracy: 0.9994\n",
      "Epoch 16/20\n",
      "85/85 [==============================] - 4s 46ms/step - loss: 3.0752e-04 - accuracy: 0.9994\n",
      "Epoch 17/20\n",
      "85/85 [==============================] - 4s 42ms/step - loss: 2.5884e-04 - accuracy: 0.9994\n",
      "Epoch 18/20\n",
      "85/85 [==============================] - 4s 42ms/step - loss: 2.1565e-04 - accuracy: 0.9993\n",
      "Epoch 19/20\n",
      "85/85 [==============================] - 3s 39ms/step - loss: 1.6388e-04 - accuracy: 0.9996\n",
      "Epoch 20/20\n",
      "85/85 [==============================] - 3s 39ms/step - loss: 2.5572e-04 - accuracy: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb7e7fb5c10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "model.fit(X_train,to_categorical(Y_train),epochs=20,batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824673029338989"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model3 = LogisticRegression(max_iter=1000)\n",
    "model3.fit(X_train,Y_train)\n",
    "model3.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14,  4, 15, ...,  9,  5, 17])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.8536349e-10, 2.2243961e-07, 6.1346178e-11, ..., 2.9780772e-06,\n",
       "        3.1289241e-05, 5.6522604e-06],\n",
       "       [3.2499225e-05, 1.0840604e-05, 4.2587548e-05, ..., 2.0947018e-06,\n",
       "        5.9984251e-10, 4.3361442e-06],\n",
       "       [3.8324406e-06, 4.8412936e-09, 9.1530467e-07, ..., 3.0120337e-11,\n",
       "        2.8917842e-07, 1.8620334e-05],\n",
       "       ...,\n",
       "       [2.9046475e-07, 3.2533630e-06, 3.1390307e-06, ..., 1.3460900e-13,\n",
       "        1.8247962e-04, 2.0319066e-08],\n",
       "       [3.8577513e-11, 3.6192978e-05, 2.1919892e-05, ..., 2.2783492e-13,\n",
       "        1.3003726e-05, 1.7833766e-11],\n",
       "       [3.6702886e-07, 1.7347826e-08, 4.8981229e-11, ..., 9.9984235e-01,\n",
       "        2.1065724e-05, 1.4597953e-05]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-40-bc459dba29cd>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([14,  4, 15, ...,  9,  5, 17])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 1s 7ms/step - loss: 0.0898 - accuracy: 0.8346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0898263156414032, 0.8345705270767212]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best\n",
    "model.evaluate(X_test,to_categorical(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 83.457%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
